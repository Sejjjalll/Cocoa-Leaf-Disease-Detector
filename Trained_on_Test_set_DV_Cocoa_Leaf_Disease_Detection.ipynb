{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Imports:\n",
        "import os, math\n",
        "from collections import Counter\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageStat\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "# For deep feature extraction using Keras ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize"
      ],
      "metadata": {
        "id": "NRAaeA6oNCwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuxCiJY8s0I9",
        "outputId": "40413269-809b-46f4-ed1c-a0c004c6a210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/archive.zip\""
      ],
      "metadata": {
        "id": "PJSH1j7Av833"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Destination directory in Colab workspace for unzipping\n",
        "extract_dir = '/content/dataset'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q \"{zip_path}\" -d \"{extract_dir}\""
      ],
      "metadata": {
        "id": "w0s5wu7EwLIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ROOT = '/content/dataset/dataset'\n",
        "train_images_dir = DATASET_ROOT + '/images/train'\n",
        "test_images_dir = DATASET_ROOT + '/images/test'\n",
        "train_labels_dir = DATASET_ROOT + '/labels/train'"
      ],
      "metadata": {
        "id": "VL-jtb9mtvie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters:\n",
        "num_classes = 3         # 0: anthracnose, 1: cssvd, 2: healthy\n",
        "global_epochs = 10      # Global epochs over the training set\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "similarity_threshold = 0.7  # For image-similarity based ground truth mapping"
      ],
      "metadata": {
        "id": "TFafVEZpyvhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For Saving model weights:\n",
        "CHECKPOINT_DIR = './checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "Gulj1SfsNPuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper Class Definitions:\n",
        "class ResidualBlock(nn.Module):\n",
        "    # ... (unchanged) ...\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride!=1 or in_channels!=out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels,out_channels,1,stride,bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self,x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "class FeatureTransformer(nn.Module):\n",
        "    # ... (unchanged) ...\n",
        "    def __init__(self, embed_dim, num_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, batch_first=True\n",
        "        )\n",
        "        self.trans = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
        "    def forward(self, x):\n",
        "        B,C,H,W = x.size()\n",
        "        xf = x.view(B,C,H*W).permute(0,2,1)    # (B,N,C)\n",
        "        xt = self.trans(xf)\n",
        "        return xt.permute(0,2,1).view(B,C,H,W)"
      ],
      "metadata": {
        "id": "8IUrvZsANTwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing and EDA:\n",
        "print(\"====== TRAINING SET EDA ======\")\n",
        "train_files = sorted(os.listdir(train_images_dir))\n",
        "print(\"Total training images:\", len(train_files))\n",
        "print(\"File types:\", Counter(os.path.splitext(f)[1] for f in train_files))\n",
        "def is_valid_image(p):\n",
        "    try:\n",
        "        with Image.open(p) as img: img.verify()\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "valid_train = [f for f in train_files\n",
        "               if is_valid_image(os.path.join(train_images_dir,f))]\n",
        "print(\"Valid training images:\", len(valid_train))\n",
        "\n",
        "print(\"\\n====== TEST SET EDA ======\")\n",
        "test_files = sorted(os.listdir(test_images_dir))\n",
        "print(\"Total test images:\", len(test_files))\n",
        "print(\"File types:\", Counter(os.path.splitext(f)[1] for f in test_files))\n",
        "valid_test = [f for f in test_files\n",
        "              if is_valid_image(os.path.join(test_images_dir,f))]\n",
        "print(\"Valid test images:\", len(valid_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NynyXKnhNYoT",
        "outputId": "302a5e8b-63a9-4a87-cae8-22a67fe9a7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== TRAINING SET EDA ======\n",
            "Total training images: 5529\n",
            "File types: Counter({'.jpg': 3661, '.jpeg': 1586, '.JPG': 282})\n",
            "Valid training images: 5529\n",
            "\n",
            "====== TEST SET EDA ======\n",
            "Total test images: 1626\n",
            "File types: Counter({'.jpg': 1079, '.jpeg': 466, '.JPG': 81})\n",
            "Valid test images: 1626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Test Ground Truth via Similarity:\n",
        "print(\"\\nExtracting deep features on train set with Keras ResNet50…\")\n",
        "kt_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "train_feats, train_lbls = [], []\n",
        "for fname in tqdm(valid_train, desc=\"Train features\"):\n",
        "    p = os.path.join(train_images_dir,fname)\n",
        "    try:\n",
        "        img = load_img(p, target_size=(224,224))\n",
        "        x   = img_to_array(img)[None]\n",
        "        x_p = preprocess_input(x)\n",
        "        feat= kt_model.predict(x_p, verbose=0).ravel()\n",
        "        train_feats.append(feat)\n",
        "        base = os.path.splitext(fname)[0]\n",
        "        lab = -1\n",
        "        lp = os.path.join(train_labels_dir, base+\".txt\")\n",
        "        if os.path.exists(lp):\n",
        "            l = open(lp).read().strip().split()[0]\n",
        "            try: lab = int(l)\n",
        "            except: lab = {\"anthracnose\":0,\"cssvd\":1,\"healthy\":2}.get(l,-1)\n",
        "        train_lbls.append(lab)\n",
        "    except Exception as e:\n",
        "        print(\"ERR train\",fname,e)\n",
        "\n",
        "train_feats = normalize(np.vstack(train_feats),axis=1)\n",
        "print(\"Built train feature bank:\", train_feats.shape[0])\n",
        "\n",
        "print(\"\\nMapping test→train by cosine similarity…\")\n",
        "test_gt={}\n",
        "for fname in tqdm(valid_test, desc=\"Test mapping\"):\n",
        "    p = os.path.join(test_images_dir,fname)\n",
        "    try:\n",
        "        img = load_img(p,target_size=(224,224))\n",
        "        x   = preprocess_input(img_to_array(img)[None])\n",
        "        f   = kt_model.predict(x,verbose=0).ravel()[None]\n",
        "        f_n = normalize(f,axis=1)\n",
        "        sim = cosine_similarity(f_n, train_feats)[0]\n",
        "        idx = sim.argmax()\n",
        "        if sim[idx]>= similarity_threshold:\n",
        "            test_gt[fname] = train_lbls[idx]\n",
        "    except Exception as e:\n",
        "        print(\"ERR test\",fname,e)\n",
        "print(\"Assigned labels to\", len(test_gt), \"test images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7Gb8BTwNb1Q",
        "outputId": "54bc5644-431a-4500-9348-a3f93068cb60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting deep features on train set with Keras ResNet50…\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train features: 100%|██████████| 5529/5529 [12:02<00:00,  7.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built train feature bank: 5529\n",
            "\n",
            "Mapping test→train by cosine similarity…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test mapping: 100%|██████████| 1626/1626 [05:58<00:00,  4.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigned labels to 1616 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Classes:\n",
        "class CocoaClassificationDataset(Dataset):\n",
        "    def __init__(self, img_dir, lbl_dir, files, transform=None):\n",
        "        self.img_dir = img_dir; self.lbl_dir=lbl_dir\n",
        "        self.files   = files;    self.tf=transform\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        fn = self.files[idx]\n",
        "        im = Image.open(os.path.join(self.img_dir,fn)).convert('RGB')\n",
        "        if self.tf: im = self.tf(im)\n",
        "        base= os.path.splitext(fn)[0]\n",
        "        lab=-1\n",
        "        lp = os.path.join(self.lbl_dir, base+\".txt\")\n",
        "        if os.path.exists(lp):\n",
        "            l=open(lp).read().strip().split()[0]\n",
        "            try: lab=int(l)\n",
        "            except: lab={\"anthracnose\":0,\"cssvd\":1,\"healthy\":2}.get(l,-1)\n",
        "        return im, lab, fn\n",
        "\n",
        "class CocoaTestDataset(Dataset):\n",
        "    def __init__(self, img_dir, gt_map, transform=None):\n",
        "        self.img_dir, self.gt_map, self.tf = img_dir, gt_map, transform\n",
        "        self.files = sorted(gt_map.keys())\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        fn = self.files[idx]\n",
        "        im = Image.open(os.path.join(self.img_dir,fn)).convert('RGB')\n",
        "        if self.tf: im = self.tf(im)\n",
        "        return im, self.gt_map[fn], fn\n",
        "\n",
        "common_tf = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_ds = CocoaClassificationDataset(\n",
        "    train_images_dir, train_labels_dir, valid_train, transform=common_tf)\n",
        "test_ds  = CocoaTestDataset(\n",
        "    test_images_dir, test_gt, transform=common_tf)"
      ],
      "metadata": {
        "id": "Jdbuc0pxNg2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transfer Learning Models:"
      ],
      "metadata": {
        "id": "r0v5g_lSOs7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50Classifier(nn.Module):\n",
        "    def __init__(self, n=num_classes):\n",
        "        super().__init__()\n",
        "        m = models.resnet50(pretrained=True)\n",
        "        m.fc = nn.Linear(m.fc.in_features, n)\n",
        "        self.net = m\n",
        "    def forward(self,x): return self.net(x)"
      ],
      "metadata": {
        "id": "tCFYdiksOG_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet121Classifier(nn.Module):\n",
        "    def __init__(self,n=num_classes):\n",
        "        super().__init__()\n",
        "        m = models.densenet121(pretrained=True)\n",
        "        m.classifier = nn.Linear(m.classifier.in_features, n)\n",
        "        self.net = m\n",
        "    def forward(self,x): return self.net(x)"
      ],
      "metadata": {
        "id": "h1PYXGHcOLgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNetB0Classifier(nn.Module):\n",
        "    def __init__(self,n=num_classes):\n",
        "        super().__init__()\n",
        "        m = models.efficientnet_b0(pretrained=True)\n",
        "        m.classifier[1] = nn.Linear(m.classifier[1].in_features,n)\n",
        "        self.net = m\n",
        "    def forward(self,x): return self.net(x)"
      ],
      "metadata": {
        "id": "GboU2g-bONor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileNetV3Classifier(nn.Module):\n",
        "    def __init__(self,n=num_classes):\n",
        "        super().__init__()\n",
        "        m = models.mobilenet_v3_large(pretrained=True)\n",
        "        m.classifier[3] = nn.Linear(m.classifier[3].in_features,n)\n",
        "        self.net = m\n",
        "    def forward(self,x): return self.net(x)"
      ],
      "metadata": {
        "id": "XjmMgSVrOP0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTClassifier(nn.Module):\n",
        "    def __init__(self,n=num_classes):\n",
        "        super().__init__()\n",
        "        m = models.vit_b_16(pretrained=True)\n",
        "        m.heads.head = nn.Linear(m.heads.head.in_features,n)\n",
        "        self.net = m\n",
        "    def forward(self,x): return self.net(x)"
      ],
      "metadata": {
        "id": "1k08dKoROSGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = {\n",
        "    \"ResNet50\"     : ResNet50Classifier(),\n",
        "    \"DenseNet121\"  : DenseNet121Classifier(),\n",
        "    \"EfficientNetB0\":EfficientNetB0Classifier(),\n",
        "    \"MobileNetV3\"  : MobileNetV3Classifier(),\n",
        "    \"ViT\"          : ViTClassifier()\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHTjvHhwOVk7",
        "outputId": "11d4876a-18ea-457d-bd67-4c3d38379b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 153MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 139MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 117MB/s] \n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 108MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 176MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and Evaluation Function:\n",
        "def train_and_evaluate(model, train_loader, test_loader, device,\n",
        "                       epochs=global_epochs, lr=learning_rate):\n",
        "    model.to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    crit= nn.CrossEntropyLoss()\n",
        "    best_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labs, _ in train_loader:\n",
        "            valid = (labs != -1).nonzero(as_tuple=True)[0]\n",
        "            if len(valid)==0: continue\n",
        "            xb, yb = imgs[valid].to(device), labs[valid].to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss= crit(out,yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_loss = running_loss/len(train_loader)\n",
        "        print(f\"Epoch {ep}/{epochs} — Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        corr=tot=0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labs, _ in test_loader:\n",
        "                xb, yb = imgs.to(device), labs.to(device)\n",
        "                out = model(xb)\n",
        "                preds = out.argmax(dim=1)\n",
        "                corr += (preds==yb).sum().item()\n",
        "                tot  += yb.size(0)\n",
        "        acc = corr/tot if tot else 0\n",
        "        print(f\"  Val Acc: {acc:.4f}\")\n",
        "\n",
        "        if acc>best_acc:\n",
        "            best_acc   = acc\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    return best_state, best_acc"
      ],
      "metadata": {
        "id": "UbjyDnEzOWU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Use test_ds for both train & eval\n",
        "train_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model_performance       = {}\n",
        "best_model_overall      = None\n",
        "best_acc_overall        = 0.0\n",
        "best_model_state_overall= None\n",
        "best_model_name         = None\n",
        "\n",
        "for name, model in model_dict.items():\n",
        "    print(f\"\\n==== Training on TEST images only: {name} ====\")\n",
        "    state, acc = train_and_evaluate(\n",
        "        model,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        device,\n",
        "        epochs=global_epochs,\n",
        "        lr=learning_rate\n",
        "    )\n",
        "    model_performance[name] = acc\n",
        "    print(f\"{name} achieved test-on-test accuracy: {acc:.4f}\")\n",
        "    # save each model's best weights\n",
        "    save_path = os.path.join(\n",
        "        CHECKPOINT_DIR,\n",
        "        f\"best_{name}_on_test.pth\"\n",
        "    )\n",
        "    torch.save(state, save_path)\n",
        "    print(f\" → Saved weights: {save_path}\")\n",
        "\n",
        "    # track overall best\n",
        "    if acc > best_acc_overall:\n",
        "        best_acc_overall        = acc\n",
        "        best_model_overall      = model\n",
        "        best_model_state_overall= state\n",
        "        best_model_name         = name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J7Bf3w5jOc7v",
        "outputId": "89fcf30b-834a-4db6-e994-118e510ae9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Training on TEST images only: ResNet50 ====\n",
            "Epoch 1/10 — Train Loss: 0.9298\n",
            "  Val Acc: 0.4920\n",
            "Epoch 2/10 — Train Loss: 0.7980\n",
            "  Val Acc: 0.6293\n",
            "Epoch 3/10 — Train Loss: 0.7292\n",
            "  Val Acc: 0.7166\n",
            "Epoch 4/10 — Train Loss: 0.6932\n",
            "  Val Acc: 0.5860\n",
            "Epoch 5/10 — Train Loss: 0.6364\n",
            "  Val Acc: 0.4975\n",
            "Epoch 6/10 — Train Loss: 0.6337\n",
            "  Val Acc: 0.7327\n",
            "Epoch 7/10 — Train Loss: 0.5934\n",
            "  Val Acc: 0.4672\n",
            "Epoch 8/10 — Train Loss: 0.5546\n",
            "  Val Acc: 0.8014\n",
            "Epoch 9/10 — Train Loss: 0.5650\n",
            "  Val Acc: 0.7438\n",
            "Epoch 10/10 — Train Loss: 0.4975\n",
            "  Val Acc: 0.8119\n",
            "ResNet50 achieved test-on-test accuracy: 0.8119\n",
            " → Saved weights: ./checkpoints/best_ResNet50_on_test.pth\n",
            "\n",
            "==== Training on TEST images only: DenseNet121 ====\n",
            "Epoch 1/10 — Train Loss: 0.8394\n",
            "  Val Acc: 0.5328\n",
            "Epoch 2/10 — Train Loss: 0.7231\n",
            "  Val Acc: 0.7209\n",
            "Epoch 3/10 — Train Loss: 0.6525\n",
            "  Val Acc: 0.8026\n",
            "Epoch 4/10 — Train Loss: 0.6324\n",
            "  Val Acc: 0.8069\n",
            "Epoch 5/10 — Train Loss: 0.6183\n",
            "  Val Acc: 0.6621\n",
            "Epoch 6/10 — Train Loss: 0.6045\n",
            "  Val Acc: 0.7723\n",
            "Epoch 7/10 — Train Loss: 0.5714\n",
            "  Val Acc: 0.7413\n",
            "Epoch 8/10 — Train Loss: 0.5608\n",
            "  Val Acc: 0.8001\n",
            "Epoch 9/10 — Train Loss: 0.5245\n",
            "  Val Acc: 0.8187\n",
            "Epoch 10/10 — Train Loss: 0.4752\n",
            "  Val Acc: 0.7240\n",
            "DenseNet121 achieved test-on-test accuracy: 0.8187\n",
            " → Saved weights: ./checkpoints/best_DenseNet121_on_test.pth\n",
            "\n",
            "==== Training on TEST images only: EfficientNetB0 ====\n",
            "Epoch 1/10 — Train Loss: 0.7260\n",
            "  Val Acc: 0.8676\n",
            "Epoch 2/10 — Train Loss: 0.4861\n",
            "  Val Acc: 0.9220\n",
            "Epoch 3/10 — Train Loss: 0.3906\n",
            "  Val Acc: 0.8750\n",
            "Epoch 4/10 — Train Loss: 0.3575\n",
            "  Val Acc: 0.9233\n",
            "Epoch 5/10 — Train Loss: 0.3043\n",
            "  Val Acc: 0.9270\n",
            "Epoch 6/10 — Train Loss: 0.2675\n",
            "  Val Acc: 0.9765\n",
            "Epoch 7/10 — Train Loss: 0.2033\n",
            "  Val Acc: 0.9226\n",
            "Epoch 8/10 — Train Loss: 0.1896\n",
            "  Val Acc: 0.9901\n",
            "Epoch 9/10 — Train Loss: 0.1533\n",
            "  Val Acc: 0.9920\n",
            "Epoch 10/10 — Train Loss: 0.1226\n",
            "  Val Acc: 0.9839\n",
            "EfficientNetB0 achieved test-on-test accuracy: 0.9920\n",
            " → Saved weights: ./checkpoints/best_EfficientNetB0_on_test.pth\n",
            "\n",
            "==== Training on TEST images only: MobileNetV3 ====\n",
            "Epoch 1/10 — Train Loss: 0.7296\n",
            "  Val Acc: 0.6918\n",
            "Epoch 2/10 — Train Loss: 0.5166\n",
            "  Val Acc: 0.6498\n",
            "Epoch 3/10 — Train Loss: 0.4066\n",
            "  Val Acc: 0.8267\n",
            "Epoch 4/10 — Train Loss: 0.3340\n",
            "  Val Acc: 0.7704\n",
            "Epoch 5/10 — Train Loss: 0.3207\n",
            "  Val Acc: 0.8484\n",
            "Epoch 6/10 — Train Loss: 0.2112\n",
            "  Val Acc: 0.9406\n",
            "Epoch 7/10 — Train Loss: 0.2327\n",
            "  Val Acc: 0.9245\n",
            "Epoch 8/10 — Train Loss: 0.1432\n",
            "  Val Acc: 0.9876\n",
            "Epoch 9/10 — Train Loss: 0.1647\n",
            "  Val Acc: 0.8843\n",
            "Epoch 10/10 — Train Loss: 0.1496\n",
            "  Val Acc: 0.8348\n",
            "MobileNetV3 achieved test-on-test accuracy: 0.9876\n",
            " → Saved weights: ./checkpoints/best_MobileNetV3_on_test.pth\n",
            "\n",
            "==== Training on TEST images only: ViT ====\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Wrong image height! Expected 224 but got 256!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-991160d8bb91>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n==== Training on TEST images only: {name} ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     state, acc = train_and_evaluate(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-142b9c10edfb>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, test_loader, device, epochs, lr)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-5e8d922ce31d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Reshape and permute the input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36m_process_input\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Wrong image height! Expected {self.image_size} but got {h}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Wrong image width! Expected {self.image_size} but got {w}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mn_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         )\n\u001b[0;32m-> 2132\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Wrong image height! Expected 224 but got 256!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance Table:\n",
        "print(\"\\n==== Model Performance (trained & eval on test_ds) ====\")\n",
        "print(f\"{'Model':<15}{'Val Accuracy':>15}\")\n",
        "print(\"-\"*30)\n",
        "for name, acc in sorted(\n",
        "        model_performance.items(),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    ):\n",
        "    print(f\"{name:<15}{acc*100:15.2f}%\")\n",
        "\n",
        "print(f\"\\nBest overall: {best_model_name} @ {best_acc_overall*100:.2f}%\")"
      ],
      "metadata": {
        "id": "7RacRVrZOjrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082a5413-f659-4647-fc4d-9abf997e4458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Model Performance (trained & eval on test_ds) ====\n",
            "Model             Val Accuracy\n",
            "------------------------------\n",
            "EfficientNetB0           99.20%\n",
            "MobileNetV3              98.76%\n",
            "DenseNet121              81.87%\n",
            "ResNet50                 81.19%\n",
            "\n",
            "Best overall: EfficientNetB0 @ 99.20%\n"
          ]
        }
      ]
    }
  ]
}